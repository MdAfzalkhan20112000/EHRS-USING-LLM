{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MdAfzalkhan20112000/EHRS-USING-LLM/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 📄 Lightweight CPU File Summarizer\n",
        "# ===============================\n",
        "\n",
        "!pip install gradio PyPDF2 python-docx\n",
        "\n",
        "import os, io, tempfile\n",
        "import PyPDF2, docx\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "import re\n",
        "import heapq\n",
        "\n",
        "# -------------------------------\n",
        "# File Readers\n",
        "# -------------------------------\n",
        "def read_txt(path_or_bytes):\n",
        "    if isinstance(path_or_bytes, (bytes, bytearray)):\n",
        "        return path_or_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
        "    with open(path_or_bytes, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def read_pdf(path_or_bytes):\n",
        "    text = []\n",
        "    if isinstance(path_or_bytes, (bytes, bytearray)):\n",
        "        reader = PyPDF2.PdfReader(io.BytesIO(path_or_bytes))\n",
        "    else:\n",
        "        reader = PyPDF2.PdfReader(path_or_bytes)\n",
        "    for page in reader.pages:\n",
        "        text.append(page.extract_text() or \"\")\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def read_docx(path_or_bytes):\n",
        "    if isinstance(path_or_bytes, (bytes, bytearray)):\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".docx\") as tmp:\n",
        "            tmp.write(path_or_bytes)\n",
        "            tmp.flush()\n",
        "            tmp_name = tmp.name\n",
        "        doc = docx.Document(tmp_name)\n",
        "        os.unlink(tmp_name)\n",
        "    else:\n",
        "        doc = docx.Document(path_or_bytes)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "def extract_text_from_file(file_obj):\n",
        "    fname = getattr(file_obj, \"name\", None)\n",
        "    if fname and os.path.exists(fname):\n",
        "        if fname.lower().endswith(\".pdf\"):\n",
        "            return read_pdf(fname)\n",
        "        elif fname.lower().endswith(\".docx\"):\n",
        "            return read_docx(fname)\n",
        "        elif fname.lower().endswith(\".txt\"):\n",
        "            return read_txt(fname)\n",
        "        else:\n",
        "            return read_txt(fname)\n",
        "    data = getattr(file_obj, \"read\", lambda: file_obj)()\n",
        "    if isinstance(data, (bytes, bytearray)):\n",
        "        if data[:4] == b\"%PDF\":\n",
        "            return read_pdf(data)\n",
        "        if data[:2] == b\"PK\":\n",
        "            return read_docx(data)\n",
        "        return read_txt(data)\n",
        "    return read_txt(str(data))\n",
        "\n",
        "# -------------------------------\n",
        "# Simple Frequency-Based Summarizer\n",
        "# -------------------------------\n",
        "def summarize_text(text, max_sentences=5):\n",
        "    # Clean text\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    sentences = re.split(r\"(?<=[.!?]) +\", text)\n",
        "\n",
        "    if len(sentences) <= max_sentences:\n",
        "        return \"⚠️ Text too short for summarization:\\n\\n\" + text\n",
        "\n",
        "    # Word frequencies\n",
        "    words = re.findall(r\"\\w+\", text.lower())\n",
        "    freq = Counter(words)\n",
        "\n",
        "    # Score sentences\n",
        "    sentence_scores = {}\n",
        "    for sent in sentences:\n",
        "        sentence_words = re.findall(r\"\\w+\", sent.lower())\n",
        "        score = sum(freq[w] for w in sentence_words if w in freq)\n",
        "        sentence_scores[sent] = score\n",
        "\n",
        "    # Pick top sentences\n",
        "    summary_sentences = heapq.nlargest(max_sentences, sentence_scores, key=sentence_scores.get)\n",
        "    summary = \" \".join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "# -------------------------------\n",
        "# Gradio Interface\n",
        "# -------------------------------\n",
        "def process_file(file):\n",
        "    try:\n",
        "        text = extract_text_from_file(file)\n",
        "        return summarize_text(text)\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error reading file: {e}\"\n",
        "\n",
        "title = \"📄 CPU-based File Summarizer\"\n",
        "desc = \"Upload a .txt, .pdf, or .docx file. Works fully on CPU (no GPU needed).\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=process_file,\n",
        "    inputs=gr.File(label=\"Upload File\", file_types=[\".txt\", \".pdf\", \".docx\"]),\n",
        "    outputs=gr.Textbox(label=\"Summary\", lines=12),\n",
        "    title=title,\n",
        "    description=desc\n",
        ")\n",
        "\n",
        "# ✅ Works in Colab with share=True\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "Y3H2umYxsjPt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}